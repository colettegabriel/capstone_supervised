{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Fraud in a Mobile Money Platform\n",
    "\n",
    "Supervised Learning Capstone\n",
    "<br>\n",
    "Colette Gabriel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Introduction to the Dataset](#data)\n",
    "* [Support Vector Machine Code](#svm)\n",
    "    - [SVM Test with best parameters from training](#test1)\n",
    "* [SGDClassifier Code](#sgd)\n",
    "    - [SGDClassifier Test with best parameters from training](#test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Dataset  <a name=\"data\"></a>\n",
    "<br>\n",
    "The dataset used in this project contains over 6 million simulated data transactions with a tiny percent (0.129%) representing fraud transactions.\n",
    "\n",
    "Because the data is so large (the excel is nearly 1/2 GB), Github recommended against storing the Excel file there. The original data for this project can be downloaded here: \n",
    "<br><br>\n",
    "Synthetic Financial Datasets For Fraud Detection\n",
    "<br>\n",
    "https://www.kaggle.com/ntnu-testimon/paysim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import imblearn\n",
    "from sklearn import svm\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/Users/colette/Documents/GitHub/credit_card_log.csv\"\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are over 10 million values for the two name fields\n",
    "#so for efficiency sake, we will drop -- but with more proccessing power this could be mined for useful data points\n",
    "df = df.drop(['nameOrig', 'nameDest'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for 'type' feature, listing the 5 types of transactions\n",
    "df = pd.concat([df, pd.get_dummies(df.type, prefix=\"type\", drop_first=True)], axis=1)\n",
    "\n",
    "dummy_column_names = list(pd.get_dummies(df.type, prefix=\"type\", drop_first=True).columns)\n",
    "\n",
    "df = df.drop('type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old and new balances are very highly correlated, so drop old and just keep new balances for orig and destination\n",
    "df = df.drop(['newbalanceOrig', 'newbalanceDest'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create filter function to filter by valid vs fraud transcations\n",
    "def cat_filter(df, category, cat_filter):\n",
    "    cat_filter = df.loc[df[category]== cat_filter]\n",
    "    return cat_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine balance of valid vs fraud transactions in the dataset\n",
    "real = cat_filter(df, 'isFraud', 0)['isFraud'].count()\n",
    "fake = cat_filter(df, 'isFraud', 1)['isFraud'].count()\n",
    "\n",
    "percent_fraud = fake/(real+fake)\n",
    "\n",
    "print('Real =', real)\n",
    "print('Fake =', fake)\n",
    "print('Percentage of transactions that are fraud: {:.6f}'.format(percent_fraud*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphically represent the tiny percentage of fraud\n",
    "names='Authentic', 'Fraud'\n",
    "size=[real, fake]\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('black')\n",
    "plt.rcParams['text.color'] = 'white'\n",
    "\n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='black')\n",
    "\n",
    "plt.pie(size, labels=names, colors=['skyblue', 'red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#step = \"Maps a unit of time in the real world. In this case 1 step is 1 hour of time.\"\n",
    "import seaborn as sns\n",
    "sns.lmplot(x='step', y='amount', hue='isFraud', data=df, fit_reg=False, legend=True)\n",
    "plt.title('Fraud vs. Valid Transactions by Amount')\n",
    "plt.xlabel('Transactions over Time')\n",
    "plt.ylabel('Amount')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at variations in 'amount'\n",
    "print('\"Amount\" stats:')\n",
    "print('Min: {:.2f}'.format(df['amount'].min()))\n",
    "print('Max: {:.2f}'.format(df['amount'].max()))\n",
    "print('# Values: {:.0f}'.format(df['amount'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at variations in 'amount'\n",
    "q75, q25 = np.percentile(df[\"amount\"], [75 ,25])\n",
    "iqr = q75 - q25\n",
    "\n",
    "for threshold in np.arange(1,30,0.5):\n",
    "    min_val = q25 - (iqr*threshold)\n",
    "    max_val = q75 + (iqr*threshold)\n",
    "    print(\"The score threshold is: {}\".format(threshold))\n",
    "    print(\"Number of outliers is: {}\".format(\n",
    "        len((np.where((df[\"amount\"] > max_val) \n",
    "                      | (df[\"amount\"] < min_val))[0]))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at variations in orig balance\n",
    "print('\"Origination Balance\" stats:')\n",
    "print('Min: {:.2f}'.format(df['oldbalanceOrg'].min()))\n",
    "print('Max: {:.2f}'.format(df['oldbalanceOrg'].max()))\n",
    "print('# Values: {:.0f}'.format(df['oldbalanceOrg'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at variations in orig balance\n",
    "q75, q25 = np.percentile(df[\"oldbalanceOrg\"], [75 ,25])\n",
    "iqr = q75 - q25\n",
    "\n",
    "for threshold in np.arange(1,30,0.5):\n",
    "    min_val = q25 - (iqr*threshold)\n",
    "    max_val = q75 + (iqr*threshold)\n",
    "    print(\"The score threshold is: {}\".format(threshold))\n",
    "    print(\"Number of outliers is: {}\".format(\n",
    "        len((np.where((df[\"oldbalanceOrg\"] > max_val) \n",
    "                      | (df[\"oldbalanceOrg\"] < min_val))[0]))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at variations in dest balance\n",
    "print('\"Destination Balance\" stats:')\n",
    "print('Min: {:.2f}'.format(df['oldbalanceDest'].min()))\n",
    "print('Max: {:.2f}'.format(df['oldbalanceDest'].max()))\n",
    "print('# Values: {:.0f}'.format(df['oldbalanceDest'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#looking at variations in dest balance\n",
    "q75, q25 = np.percentile(df[\"oldbalanceDest\"], [75 ,25])\n",
    "iqr = q75 - q25\n",
    "\n",
    "for threshold in np.arange(1,30,0.5):\n",
    "    min_val = q25 - (iqr*threshold)\n",
    "    max_val = q75 + (iqr*threshold)\n",
    "    print(\"The score threshold is: {}\".format(threshold))\n",
    "    print(\"Number of outliers is: {}\".format(\n",
    "        len((np.where((df[\"oldbalanceDest\"] > max_val) \n",
    "                      | (df[\"oldbalanceDest\"] < min_val))[0]))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create new colums for log transformed values of amount, origination balance, and destination balance\n",
    "#plot before and after\n",
    "df['amount_log'] = np.log(df.amount+1)\n",
    "df['origBalance_log'] = np.log(df.oldbalanceOrg+1)\n",
    "df['destBalance_log'] = np.log(df.oldbalanceDest+1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# histograms of the box-cox transformed data\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.hist(df.amount, facecolor='k', alpha=0.75)\n",
    "plt.title('Histogram Amounts')\n",
    "plt.ylabel('Amount')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.hist(df.amount_log, facecolor='k', alpha=0.75)\n",
    "plt.title('Amounts (log transformed)')\n",
    "plt.ylabel('Amount')\n",
    "plt.grid(True)\n",
    "\n",
    "# histograms of the box-cox transformed data\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.hist(df.oldbalanceOrg, facecolor='k', alpha=0.75)\n",
    "plt.title('Histogram Origination Balance')\n",
    "plt.ylabel('Amount')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.hist(df.origBalance_log, facecolor='k', alpha=0.75)\n",
    "plt.title('Origination Balance (log transformed)')\n",
    "plt.ylabel('Amount')\n",
    "plt.grid(True)\n",
    "\n",
    "# histograms of the box-cox transformed data\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.hist(df.oldbalanceDest, facecolor='k', alpha=0.75)\n",
    "plt.title('Histogram Destination Balance')\n",
    "plt.ylabel('Amount')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.hist(df.destBalance_log, facecolor='k', alpha=0.75)\n",
    "plt.title('Destination Balance (log transformed)')\n",
    "plt.ylabel('Amount')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under sampling method - generate multiple small datasets of 16,426 transactions\n",
    "#use 80% of fraud transactions (with 20% reserved as a final test set) & same number of random valid samples\n",
    "\n",
    "fraud_count = len(df[df.isFraud == 1])\n",
    "fraud_sample_count = int(len(df[df.isFraud == 1])*0.8)\n",
    "\n",
    "#collect all valid transactions for random use in balanced datasets\n",
    "valid_indices = df[df.isFraud == 0].index\n",
    "\n",
    "#collect all fraud transactions and set 20% aside for a final test\n",
    "fraud_indices = df[df.isFraud == 1].index\n",
    "fraud_train = fraud_indices[:fraud_sample_count]  #6570 samples used in every training dataset\n",
    "fraud_test = fraud_indices[fraud_sample_count:]   #1643 samples set aside for a final test dataset\n",
    "\n",
    "data_num = 25\n",
    "data_sample_under = dict()\n",
    "\n",
    "#generate small random datasets for training purposes using UNDERSAMPLING\n",
    "for i in range (0,data_num):\n",
    "    random_ind_under = np.random.choice(valid_indices, fraud_sample_count, replace=False)  #random sample of valid indices\n",
    "    under_sample_ind = np.concatenate([fraud_train, random_ind_under])\n",
    "    data_sample_under[i] = df.loc[under_sample_ind]\n",
    "    \n",
    "#generate final testing dataset\n",
    "random_ind_under = np.random.choice(valid_indices, math.ceil(fraud_count*0.2), replace=False)  #random sample of valid indices\n",
    "under_sample_ind = np.concatenate([fraud_test, random_ind_under])\n",
    "testing_sample = df.loc[under_sample_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that sample sets are evenly balanced\n",
    "real = cat_filter(data_sample_under[0], 'isFraud', 0)['isFraud'].count()\n",
    "fake = cat_filter(data_sample_under[0], 'isFraud', 1)['isFraud'].count()\n",
    "percent_fraud = fake/(real+fake)\n",
    "\n",
    "print('Real =', real)\n",
    "print('Fake =', fake)\n",
    "print('Percentage of transactions that are fraud: {:.6f}'.format(percent_fraud*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats for the final true test dataset (20% of the actual fraud transactions that were held in reserve)\n",
    "real = cat_filter(testing_sample, 'isFraud', 0)['isFraud'].count()\n",
    "fake = cat_filter(testing_sample, 'isFraud', 1)['isFraud'].count()\n",
    "percent_fraud = fake/(real+fake)\n",
    "\n",
    "print('Real =', real)\n",
    "print('Fake =', fake)\n",
    "print('Percentage of transactions that are fraud: {:.6f}'.format(percent_fraud*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample_under[24].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "obs_train = dict()\n",
    "obs_test = dict()\n",
    "matrix = dict()\n",
    "report = dict()\n",
    "tn = dict()\n",
    "fp = dict()\n",
    "fn = dict()\n",
    "tp = dict()\n",
    "acc = dict()\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "f1 = dict()\n",
    "X = dict()\n",
    "y = dict()\n",
    "\n",
    "for i in range(0, len(data_sample_under)):\n",
    "    X[i] = data_sample_under[i].drop(columns=['isFraud', 'amount', 'oldbalanceOrg', 'oldbalanceDest'])\n",
    "    y[i] = data_sample_under[i]['isFraud']\n",
    "    X[i] = scaling.fit_transform(X[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine code <a name=\"svm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = LinearSVC(max_iter=2000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*(precision[i]*recall[i]))/((precision[i] + recall[i]))\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision-recall curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          (total_prec*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = LinearSVC(max_iter=3000, tol=1e-5, loss='hinge')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision-recall curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          (total_prec*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SVC(kernel='linear', C=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision-recall curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          (total_prec*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SVC(kernel='sigmoid')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "#parameter tuning with GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001, 0.00001, 10]}\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf_grid = GridSearchCV(svm.SVC(), param_grid, verbose=1)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    y_pred = clf_grid.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*(precision[i]*recall[i]))/((precision[i] + recall[i]))\n",
    "    print(\"Dataset {} Best Parameters: {}\".format(i, clf_grid.best_params_))\n",
    "    print(\"\\nDataset {} Best Estimators: {}\".format(i, clf_grid.best_estimator_))\n",
    " \n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, \n",
    "              decision_function_shape='ovr', degree=3, gamma=10, kernel='rbf',\n",
    "              max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, \n",
    "              decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
    "              max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3,\n",
    "              gamma=1, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision-recall curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          (total_prec*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, \n",
    "              gamma=10, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3,\n",
    "              gamma=1, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001,\n",
    "              verbose=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    #(2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = svm.SVC(kernel='linear', C=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = svm.SVC(kernel='linear', C=1.0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Test with best parameters from training <a name=\"test1\"></a>\n",
    "<br>\n",
    "Run the best performing support machine vector algorithms with the final reserved test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_finalTest = testing_sample.drop(columns=['isFraud', 'amount', 'oldbalanceOrg', 'oldbalanceDest'])\n",
    "y_finalTest = testing_sample['isFraud']\n",
    "X_finalTest = scaling.fit_transform(X_finalTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "pca.fit(X_finalTest)\n",
    "obs_test = X_finalTest.shape\n",
    "clf = svm.SVC(C=100, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3,\n",
    "        gamma=1, kernel='rbf', probability=False, random_state=None, shrinking=True, tol=0.001,\n",
    "        verbose=False)\n",
    "clf.fit(X_finalTest, y_finalTest)\n",
    "y_pred = clf.predict(X_finalTest)\n",
    "matrix = confusion_matrix(y_finalTest, y_pred)\n",
    "report = classification_report(y_finalTest, y_pred)\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = confusion_matrix(y_finalTest, y_pred).ravel()\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "acc = metrics.accuracy_score(y_finalTest, y_pred)\n",
    "#Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "precision = tp/(tp+fp) \n",
    "#Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "recall = tp/(tp+fn) \n",
    "#F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "f1 = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of observations in this final test set is {}\".format(obs_test))\n",
    "print(\"Confusion Matrix\\n\", matrix)\n",
    "print(\"Classification report\\n\", report)\n",
    "print(\"\\nTrue Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "print(\"Accuracy: \",acc)\n",
    "print(\"Precision {:0.2f}\".format(precision))\n",
    "print(\"Recall {:0.2f}\".format(recall))\n",
    "print(\"F1 Score {:0.2f}\".format(f1))\n",
    "print(\"Sklearn F1 {:0.2f}\".format(f1_score(y_finalTest, y_pred, average='macro')))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "pca.fit(X_finalTest)\n",
    "obs_test = X_finalTest.shape\n",
    "clf = svm.SVC(gamma='scale', decision_function_shape='ovo')\n",
    "clf.fit(X_finalTest, y_finalTest)\n",
    "y_pred = clf.predict(X_finalTest)\n",
    "matrix = confusion_matrix(y_finalTest, y_pred)\n",
    "report = classification_report(y_finalTest, y_pred)\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = confusion_matrix(y_finalTest, y_pred).ravel()\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "acc = metrics.accuracy_score(y_finalTest, y_pred)\n",
    "#Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "precision = tp/(tp+fp) \n",
    "#Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "recall = tp/(tp+fn) \n",
    "#F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "f1 = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of observations in this final test set is {}\".format(obs_test))\n",
    "print(\"Confusion Matrix\\n\", matrix)\n",
    "print(\"Classification report\\n\", report)\n",
    "print(\"\\nTrue Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "print(\"Accuracy: \",acc)\n",
    "print(\"Precision {:0.2f}\".format(precision))\n",
    "print(\"Recall {:0.2f}\".format(recall))\n",
    "print(\"F1 Score {:0.2f}\".format(f1))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier code <a name=\"sgd\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "#parameter tuning with GridSearchCV\n",
    "param_grid = {\"n_iter_no_change\": [1, 5, 10],\n",
    "              \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              \"penalty\": [\"none\", \"l1\", \"l2\"]}\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf_grid = GridSearchCV(SGDClassifier(), param_grid, verbose=1)\n",
    "    #clf = SGDClassifier()\n",
    "    #clf_grid = GridSearchCV(clf, param_grid=param_grid)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    y_pred = clf_grid.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*(precision[i]*recall[i]))/((precision[i] + recall[i]))\n",
    "    print(\"Dataset {} Best Parameters: {}\".format(i, clf_grid.best_params_))\n",
    "    print(\"\\nDataset {} Best Estimators: {}\".format(i, clf_grid.best_estimator_))\n",
    " \n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SGDClassifier(tol=1e-3, shuffle=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, \n",
    "                        epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
    "                        loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='none', power_t=0.5,\n",
    "                        random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
    "                        warm_start=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SGDClassifier(tol=1e-3, shuffle=True, loss='log')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    pca.fit(X[i])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[i], y[i], test_size=0.25, random_state=0)\n",
    "    obs_train[i] = X_train.shape\n",
    "    obs_test[i] = X_test.shape\n",
    "    clf = SGDClassifier(alpha=0.01, average=False, class_weight=None, early_stopping=False, epsilon=0.1, \n",
    "                        eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', \n",
    "                        max_iter=1000, n_iter_no_change=1, n_jobs=None, penalty='none', power_t=0.5, \n",
    "                        random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
    "                        warm_start=False)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    matrix[i] = confusion_matrix(y_test, y_pred)\n",
    "    report[i] = classification_report(y_test, y_pred)\n",
    "    #extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "    tn[i], fp[i], fn[i], tp[i] = confusion_matrix(y_test, y_pred).ravel()\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    acc[i] = metrics.accuracy_score(y_test, y_pred)\n",
    "    #Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "    precision[i] = tp[i]/(tp[i]+fp[i]) \n",
    "    #Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "    recall[i] = tp[i]/(tp[i]+fn[i]) \n",
    "    #F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "    f1[i] = (2*precision[i]*recall[i])/(precision[i] + recall[i])\n",
    "    \n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = 0\n",
    "total_recall = 0\n",
    "total_prec = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for i in range(0, len(X)):\n",
    "    print(\"The number of observations in {} training set is {}\".format(i, obs_train[i]))\n",
    "    print(\"The number of observations in test set is {}\".format(obs_test[i]))\n",
    "    print(\"Confusion Matrix\\n\", matrix[i])\n",
    "    print(\"Classification report\\n\", report[i])\n",
    "    print(\"\\nTrue Negatives: \",tn[i])\n",
    "    print(\"False Positives: \",fp[i])\n",
    "    print(\"False Negatives: \",fn[i])\n",
    "    print(\"True Positives: \",tp[i])\n",
    "    print(\"Accuracy: \",acc[i])\n",
    "    print(\"Precision {:0.2f}\".format(precision[i]))\n",
    "    print(\"Recall {:0.2f}\".format(recall[i]))\n",
    "    print(\"F1 Score {:0.2f}\".format(f1[i]))\n",
    "    print(\"\\n\\n\")\n",
    "    total_acc = total_acc + acc[i]\n",
    "    total_recall = total_recall + recall[i]\n",
    "    total_prec = total_prec + precision[i]\n",
    "    total_f1 = total_f1 + f1[i]\n",
    "    \n",
    "print(\"Ave accuracy = {:0.2f}\".format((total_acc*100)/len(X)))\n",
    "print(\"Ave recall = {:0.2f}\".format((total_recall*100)/len(X)))\n",
    "print(\"Ave precision = {:0.2f}\".format((total_prec*100)/len(X)))\n",
    "print(\"Ave F1 = {:0.2f}\".format((total_f1*100)/len(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Test with best parameters from training <a name=\"test2\"></a>\n",
    "<br>\n",
    "Run the best performing SGD algorithms with the final reserved test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_finalTest = testing_sample.drop(columns=['isFraud', 'amount', 'oldbalanceOrg', 'oldbalanceDest'])\n",
    "y_finalTest = testing_sample['isFraud']\n",
    "X_finalTest = scaling.fit_transform(X_finalTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "pca.fit(X_finalTest)\n",
    "obs_test = X_finalTest.shape\n",
    "clf = SGDClassifier(tol=1e-3, shuffle=True)\n",
    "clf.fit(X_finalTest, y_finalTest)\n",
    "y_pred = clf.predict(X_finalTest)\n",
    "matrix = confusion_matrix(y_finalTest, y_pred)\n",
    "report = classification_report(y_finalTest, y_pred)\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = confusion_matrix(y_finalTest, y_pred).ravel()\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "acc = metrics.accuracy_score(y_finalTest, y_pred)\n",
    "#Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "precision = tp/(tp+fp) \n",
    "#Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "recall = tp/(tp+fn) \n",
    "#F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "f1 = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of observations in this final test set is {}\".format(obs_test))\n",
    "print(\"Confusion Matrix\\n\", matrix)\n",
    "print(\"Classification report\\n\", report)\n",
    "print(\"\\nTrue Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "print(\"Accuracy: \",acc)\n",
    "print(\"Precision {:0.2f}\".format(precision))\n",
    "print(\"Recall {:0.2f}\".format(recall))\n",
    "print(\"F1 Score {:0.2f}\".format(f1))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working...')\n",
    "start_time = time.time()\n",
    "\n",
    "pca.fit(X_finalTest)\n",
    "obs_test = X_finalTest.shape\n",
    "clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0,\n",
    "                    fit_intercept=True, l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
    "                    n_iter_no_change=5, n_jobs=None, penalty='none', power_t=0.5, random_state=None,\n",
    "                    shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "clf.fit(X_finalTest, y_finalTest)\n",
    "y_pred = clf.predict(X_finalTest)\n",
    "matrix = confusion_matrix(y_finalTest, y_pred)\n",
    "report = classification_report(y_finalTest, y_pred)\n",
    "\n",
    "#extracting true_positives, false_positives, true_negatives, false_negatives\n",
    "tn, fp, fn, tp = confusion_matrix(y_finalTest, y_pred).ravel()\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "acc = metrics.accuracy_score(y_finalTest, y_pred)\n",
    "#Precision - ‘Exactness’, ability of the model to return only relevant instances (ex. minimizing false positives)\n",
    "precision = tp/(tp+fp) \n",
    "#Recall - ‘Completeness’, ability of the model to identify all relevant instances, True Positive Rate, aka Sensitivity.\n",
    "recall = tp/(tp+fn) \n",
    "#F1 Score - Harmonic mean of Precision & Recall, used to indicate a balance between Precision & Recall providing each equal weightage, it ranges from 0 to 1\n",
    "f1 = (2*precision*recall)/(precision + recall)\n",
    "\n",
    "print(\"--- %s seconds ---\" % round(time.time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of observations in this final test set is {}\".format(obs_test))\n",
    "print(\"Confusion Matrix\\n\", matrix)\n",
    "print(\"Classification report\\n\", report)\n",
    "print(\"\\nTrue Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "print(\"Accuracy: \",acc)\n",
    "print(\"Precision {:0.2f}\".format(precision))\n",
    "print(\"Recall {:0.2f}\".format(recall))\n",
    "print(\"F1 Score {:0.2f}\".format(f1))\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
